# Realized Volatility Forecasting for New Issues and Spin-Offs using Multi-Source Transfer Learning

**Date:** December 2025

**Authors:** 
- Andreas Teller (andreas.teller@uni-jena.de) 

**Execution Environment:** Linux / 4-core Intel® Core™ i7 CPU  

**Runtime:** Running the full set of experiments end-to-end requires approximately one week of computation time on this machine.

This repository implements a forecasting pipeline for realized volatility, with a focus on new issues and spin-offs. It combines traditional econometric models and machine learning approaches with multi-source transfer learning techniques to improve predictive performance in data-scarce settings.

## Data

### Stock Market Data
High-frequency stock market data must be **purchased from FirstRate Data**.

- Provider: FirstRate Data  
- Website: https://www.firstratedata.com  
- Dataset: Complete Intraday Bundle

Please ensure that the downloaded data is stored in: `data/raw/stock_data/`

---

### Earnings Announcement Data
Earnings announcement dates can be obtained from **Yahoo Finance**.

- Source: Yahoo Finance  
- Website: https://finance.yahoo.com  

These data should be stored in: `data/raw/earning_anounce_data/`


---

### Macroeconomic Data
Macroeconomic and financial uncertainty indicators are sourced from publicly available providers:

- **US 3-Month Treasury Bill Rate (US3M)**  
  https://fred.stlouisfed.org/series/DTB3

- **Aruba–Diebold–Scotti Business Conditions Index (ADS)**  
  https://www.philadelphiafed.org/surveys-and-data/real-time-data-research/ads

- **Economic Policy Uncertainty Index (EPU)**  
  https://www.policyuncertainty.com

- **CBOE Volatility Index (VIX)**  
  https://www.cboe.com/tradable_products/vix/vix_historical_data

- **Hang Seng Index (HSI)**  
  https://finance.yahoo.com

All macroeconomic data files should be placed in: `data/raw/macro_data/`


## Usage

### Overview

The pipeline consists of three main steps:
1. **Data Processing** - Process raw financial data into features
2. **Model Predictions** - Train models and generate predictions
3. **Report Generation** - Analyze results and create reports

---

### 1. Raw Data Processing

#### Script: `run_raw_data_processing.py`

This script processes raw stock market data and macro-economic indicators into features.

#### Prerequisites

Before running, ensure you have:
- Raw stock data in `data/raw/stock_data/`
- Raw macro-economic data in `data/raw/macro_data/`
- Earnings announcement data in `data/raw/earning_announce_data/`

#### Running the Script

```bash
python run_raw_data_processing.py
```

#### Output Structure

```
data/processed/
├── 1/
│   ├── std/
│   │   ├── AAPL.csv
│   │   └── ...
│   ├── ext/
│   ├── std-q/
│   ├── std-semi/
│   ├── ext-q/
│   └── ext-semi/
├── 5/
│   ├── std/
│   ├── ext/
│   ├── std-q/
│   ├── std-semi/
│   ├── ext-q/
│   └── ext-semi/
└── 22/
    ├── std/
    ├── ext/
    ├── std-q/
    ├── std-semi/
    ├── std-mean/
    ├── ext-q/
    ├── ext-semi/
    └── ext-mean/
```

---

### 2. Model Predictions

#### Script: `run_model_predictions.py`

This script trains volatility prediction models and generates forecasts. 

#### Prerequisites

- Processed data files (generated by `run_raw_data_processing.py`)

#### Running the Script

```bash
python run_model_predictions.py --model har --tl_method target-only --backtest_start_offset 50 --predictor_set std --retrain_intervall 5
```

#### Command-Line Arguments

| Argument | Options | Description |
|----------|---------|-------------|
| `--model` | `random-walk`, `har`, `xgboost`, `feedforward-neural-network`, `random-forest`, `lasso` | Forecast model to use |
| `--tl_method` | `target-only`, `naive-pooling`, `mtl` | Transfer learning method |
| `--backtest_start_offset` | `1`, `5`, `22`, `50` | Backtest start offset (in days) from initial trading day |
| `--predictor_set` | `std`, `ext`, `std-q`, `std-semi`, `std-mean`, `ext-q`, `ext-semi`, `ext-mean` | Predictor set type |
| `--tl_perc` | `25`, `50`, `75` | Percentile threshold for MTL DTW filtering (required when `--tl_method` is `mtl`) |
| `--retrain_intervall` | `1`, `5` | After how many days the model should be retrained |

#### Argument Details

**`--model`**
- `random-walk`: Baseline random walk model
- `har`: Heterogeneous Autoregressive model
- `xgboost`: Extreme gradient boosting model
- `feedforward-neural-network`: Feedforward neural network
- `random-forest`: Random forest 
- `lasso`: Lasso regression with

**`--tl_method`**
- `target-only`: Train only on target asset data (no transfer learning)
- `naive-pooling`: Pool all source and target data together
- `mtl`: Multi source transfer learning with DTW-based instance selection (requires `--tl_perc`)

**`--backtest_start_offset`**
- `1`
- `5`
- `22`
- `50`

**`--predictor_set`**
- `std`: Standard features (HAR components only)
- `ext`: Extended features 
- `std-q`: Standard features with realized quarticity 
- `std-semi`: Standard features with semivariance adjustments
- `ext-q`: Extended features with realized quarticity 
- `ext-semi`: Extended features with semivariance adjustments
- `std-mean`: Standard features supplemented by the average rv across source assets and target asset  
- `ext-mean`: Extended features supplemented by the average rv across source assets and target asset  

**`--tl_perc`**
- Only required when using `--tl_method mtl`
- Lower percentiles = more selective instance selection
- Higher percentiles = more source instances included

**`--retrain_intervall`**
- `1`: Retrain model daily
- `5`: Retrain model every 5 days

#### Output Files

- Results are saved to `results/predictions/`:

- Metadata for MTL runs is saved to `results/meta/`.

---

### 3. Report Generation

#### Script: `run_report_generation.py`

This script analyzes model predictions and generates evaluation reports with performance metrics and visualizations.

#### Prerequisites

- Prediction files (generated by `run_model_predictions.py`)

#### Running the Script

```bash
python run_report_generation.py
```

#### Output Files

Tables are saved to `reports/`:

Figures are saved to `figures/`:

---

## Dependencies

This project relies on a set of Python packages for data processing, modeling, and evaluation.

The codebase was developed and executed using **Python 3.11**. All required packages and versions are listed in the `requirements.txt` file. Please refer to that file to ensure a compatible environment when installing dependencies.

You can install all dependencies with:

```bash
pip install -r requirements.txt